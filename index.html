<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Targeting the distribution gap via data augmentation</title>

    <link rel="stylesheet" href="./css/main.css" />
  </head>
  <body>
    <h1>
      Targeting the distribution gap by augmentation
    </h1>
        <strong>Researchers: </strong>Sarah Danzi, Jennifer Mahle,<a href="https://www.linkedin.com/in/boudreauxjohn/"> John Boudreaux</a>
        <br>
    A UC Berkeley Master's in Information and Data Science capstone research Project
    <br>
    <br>
    <h2>Abstact</h2>
    <br>
    Computer vision algorithms suffer from a distribution gap when using common datasets, meaning that performance on a given dataset may not transfer over to a new one (i.e., CIFAR-10 to CIFAR-10.1). In this work, we explore using data augmentation as a means to lessen the distribution gap. Augmentation, using the RandAugment and CutMix methods, is used to create new datasets based off of CIFAR-10 and then used for training various models from literature. The models are then trained with a smaller learning rate for 50 epochs on the original CIFAR-10 data. Models are then evaluated on their performance on CIFAR-10 and CIFAR-10.1 test sets, and the difference between these is the distribution gap. The augmentation method does not appear to provide any benefits for the distribution gap and worsens overall accuracy.
    <br>
    <h2>Background</h2>
    <br>
    Image recognition research and training is often based on performance on a few different academic standard datasets like CIFAR-10 or ImageNet. While these give the advantage of being able to compare models to each other for performance metrics, training consistently on the same dataset introduces the question: "How much can we trust these classifiers in the real world?" There is even reason to believe that they might not even generalize to the data used to generate their training and evaluation datasets. Recent work from UC Berkeley (Recht et. al) generated a new dataset using identical protocol to that of CIFAR-10 and observed that classifiers performed notably worse on the newly created CIFAR-10.2 dataset. They attribute this difference to a distribution gap, meaning that images between datasets are inherently different in their distributions. A question of interest is how to train models to lessen this distribution gap and hopefully have better performance in real world applications.
    <br>
    <br>
    Research from He et al. (2019) indicates that data augmentation can be a feasible regularization approach. The goal of doing this is to help models only focus on most important hallmark features of each class rather than idiosyncracies of the distribution of data available for training. While this work did show improved results for overall model accuracy, there was no attention to the distribution gap. This work will continue on He et al., where we will analyze this approach and its applicability in lessening the distribution gap.

    <br>
    <h2>Methods</h2>
    <br>
    <h4>Models</h4>
    <br>
    The models tested for this research included densenet, resnet, resnext, and wide residual networks (wrn). These are various datasets that have performed well on the CIRFAR-10 dataset in academic literature. Training protocol and architecture follows that of the papers in which they were published.
    <br>
    <br>
    <h4>Augmentation Methods</h4>
    The primary augmentation method utlizied to create new training datasets was RandAugment. This method takes two hyperparameters- <i>N</i>, the number of transformations to apply randomly, and <i>M</i>, the magnitude of the individual transformations. RandAugment randomly selects and applies transformations like blocking parts of the image, flipping, warping, and changing contrast. Some examples of original data and the augmented data created by RandAugment can be seen below for a few different hyperparametes in <i>Figure 1.</i>

    <div class="image">
      <img src="https://raw.githubusercontent.com/boudrejp/mids-capstone/master/img/augmentation_sample.JPG">
    </div>
    <div class="image-caption">
      <strong>Figure 1.</strong> Sample transformations using RandAugment. Images are significantly changed with higher N and M values.
    </div>
    <br>
    <br>
    Another augmentation method used was CutMix. CutMix samples images randomly from a dataset and will merge two images together. The resulting image has two different labels, each which relate to a different proportion of the image. An example CutMix application can be seen below in <i>Figure 2</i>.
    <br>
    <div class="image">
      <img src="https://raw.githubusercontent.com/boudrejp/mids-capstone/master/img/cutmix_example.JPG">
    </div>
    <div class="image-caption">
      <strong>Figure 2.</strong> Sample CutMix and related techniques. CutMix results in an image with two labels that add to 1.
    </div>
    <br>
    <h2>Experiments</h2>
    <br>
    <h4>RandAugment and refinement</h4>
    <br>
    <br>
    The main experiment in this research involved a few different steps. First, each relevant model was trained according to its relevant protocol in its publication. Accuracy and loss were noted on both CIFAR-10 and CIFAR-10.1 test sets. Each model type was then retrained independently on 3 different RandAugment datasets with varying hyperparameters, and these were then scored on CIFAR-10 and CIFAR-10.1 test sets. Hyperparameter setups for RandAugment datasets can be seen in <i>Table 1.</i>
    <br>
    <br>
      <table class = "tg">
<thead>
  <tr>
    <th class="tg-0lax">Experiment setup</th>
    <th class="tg-0lax">N (number of tranformations)</th>
    <th class="tg-0lax">M (Magnitude of Transformations)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Original</td>
    <td class="tg-0lax">0</td>
    <td class="tg-0lax">0</td>
  </tr>
  <tr>
    <td class="tg-0lax">A</td>
    <td class="tg-0lax">2</td>
    <td class="tg-0lax">5</td>
  </tr>
  <tr>
    <td class="tg-0lax">B</td>
    <td class="tg-0lax">2</td>
    <td class="tg-0lax">20</td>
  </tr>
  <tr>
    <td class="tg-0lax">C</td>
    <td class="tg-0lax">3</td>
    <td class="tg-0lax">20</td>
  </tr>
</tbody>
</table>
    <div class="image-caption">
      <strong>Table 1.</strong> Table of RandAugment dataset configurations
    </div>
    <br>
    <br>
    <div class="image">
      <img src="https://raw.githubusercontent.com/boudrejp/mids-capstone/master/img/resnet_ra_probabilities.JPG">
    </div>
    <div class="image-caption">
      <strong>Figure X.</strong> What happens when we do a ton of augmentation to resnet
    </div>

    <br>
    <h2>Results</h2>
    <br>
    <br>
    <h2>Analysis</h2>
    <br>
    <h4>Subheading 1</h4>
    <br>
    <br>
    <h4>Subheading 2</h4>
    <br>
    <br>
    <h2>Follow-up</h2>
    <br>
    <br>
    <h2>Conclusions</h2>
    <br>
    <br>
    <!--<h2>Thanks</h2> no spoilers! -->
    <br>
    <br>
    <h2>Citations</h2>
    <ul>
      <li>Cubuk, E., Zoph, B., Mane, D., Vasudevan, V., & Le, Q. (2019). AutoAugment: Learning Augmentation Strategies From Data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</li>
      <li>Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, & Quoc V. Le. (2019). RandAugment: Practical automated data augmentation with a reduced search space.</li>
      <li>He, Zhuoxun & Xie, Lingxi & Chen, Xin & Zhang, Ya & Wang, Yanfeng & Tian, Qi. (2019). Data Augmentation Revisited: Rethinking the Distribution Gap between Clean and Augmented Data.</li>
      <li>Recht, Benjamin, Roelofs, Rebecca, Schmidt, Ludwig, & Shankar, Vaishaal (2019).  Do ImageNet Classifiers Generalize to ImageNet?CoRR, abs/1902.10811.</li>
      <li>Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, & Wieland Brendel. (2018). ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.</li>
      <li>Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In International Conference on Computer Vision, 2019.</li>
    </ul>

        <script src="./js/main.js"></script>
  </body>
</html>
